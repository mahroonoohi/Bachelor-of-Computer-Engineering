Scaling Properties of Speech Language Models
Santiago Cuervo andRicard Marxer
Université de Toulon, Aix Marseille Université, CNRS, LIS, France
{santiago.cuervo, ricard.marxer}@lis-lab.fr
Abstract
Speech Language Models (SLMs) aim to learn
language from raw audio, without textual re-
sources. Despite significant advances, our cur-
rent models exhibit weak syntax and semantic
abilities. However, if the scaling properties of
neural language models hold for the speech
modality, these abilities will improve as the
amount of compute used for training increases.
In this paper, we use models of this scaling
behavior to estimate the scale at which our cur-
rent methods will yield a SLM with the English
proficiency of text-based Large Language Mod-
els (LLMs). We establish a strong correlation
between pre-training loss and downstream syn-
tactic and semantic performance in SLMs and
LLMs, which results in predictable scaling of
linguistic performance. We show that the lin-
guistic performance of SLMs scales up to three
orders of magnitude more slowly than that of
text-based LLMs. Additionally, we study the
benefits of synthetic data designed to boost se-
mantic understanding and the effects of coarser
speech tokenization.
1 Introduction
Inspired by the remarkable ability of preschool
children to learn language from raw sensory in-
puts, Lakhotia et al. (2021) introduced in their sem-
inal paper the textless NLP (Natural Language Pro-
cessing) project. The project aimed to leverage
advances in self-supervised speech representation
learning for unsupervised unit discovery (Hsu et al.,
2021; Chung et al., 2021) and generative neural
language models (Brown et al., 2020) to jointly
learn the acoustic and linguistic characteristics of
a language from audio alone, without access to
textual supervision (e.g. lexicon or transcriptions).
They formalized this goal in the task of Genera-
tive Spoken Language Modeling (GSLM), in which
a language model is trained on sequences of self-
supervised learned speech units.
Despite a significant body of research on these
Figure 1: Speech Language Models test loss curves for
all our single-epoch runs. Axes are in logarithmic scale.
The envelope of minimal loss per FLOP (black dots)
follows a power law (dashed line).
speech-based language models (SLMs) (Lakhotia
et al., 2021; Kharitonov et al., 2022; Borsos et al.,
2023; Hassid et al., 2023), they are still far from
matching the syntactic and semantic abilities of
text-based systems (Hassid et al., 2023). Therefore,
the promise of textless NLP is yet to be realized.
However, if the scaling behavior of text-based neu-
ral language models (Brown et al., 2020; Kaplan
et al., 2020) holds for the speech modality, we can
reasonably expect those abilities to improve as the
amount of compute used for training increases.
In this work, we apply recently proposed models
of the scaling behavior of neural language models
to SLMs, and use them to estimate the scale at
which our current methods will match the linguistic
performance of Large Language Models (LLMs),
generative text-based systems that have achieved
remarkably strong performance across a wide range
of NLP applications (Brown et al., 2020). The main
contributions of this work are:
•We trained over 50 SLMs with different num-arXiv:2404.00685v1  [eess.AS]  31 Mar 2024Figure 2: Downstream linguistic performance scaling with compute for LLMs and SLMs. Axes are in logarithmic
scale. Syntactic (BLIMP) and semantic (Topic Cloze and Story Cloze) metrics follow a power law before starting to
saturate. Linguistic performance scales up to three orders of magnitude more slowly in SLMs relative to LLMs.
ber of parameters and data budgets. We show
that the test loss of SLMs follows scaling
power laws as those observed in text-based
LLMs (Figure 1), and use the methods from
Hoffmann et al. (2022) and Muennighoff et al.
(2023) to model the scaling behavior of SLMs.
•We establish a strong correlation between the
test loss of neural LMs and the downstream
metrics commonly used to evaluate their syn-
tactic and semantic abilities. Therefore, the
linguistic performance of LMs follows similar
scaling laws (Figure 2). We leverage this in-
sight to determine the relative efficiency with
scale of SLMs relative to LLMs.
•We speculate that SLMs require more context
than fits in their context window to acquire
from commonly used speech datasets the se-
mantic understanding measured by our met-
rics. Accordingly, we propose a new speech
dataset to boost semantic understanding in
SLMs. Specifically, we synthesized a spo-
ken version of the Tiny Stories dataset (Eldan
and Li, 2023), and show that its use during
pre-training improves semantic downstream
performance.
•On the basis of our previous observation, we
studied the use of unigram tokenization to
shorten sequences and pack more information
in the context window of SLMs. However,
our results suggest that a coarser tokenization
is detrimental to downstream performance.
2 Background
2.1 Generative spoken language modeling
We follow the GSLM framework from Lakhotia
et al. (2021). The general GSLM pipeline is com-posed of three separately trained models: (i) a
speech tokenizer, (ii) a language model, and (iii) a
vocoder (token-to-waveform) module. In the fol-
lowing, we provide background for the speech tok-
enizer and LM, as these are the components we use
in this work. For details about the vocoder please
refer to Lakhotia et al. (2021).
Speech tokenizers transform raw speech wave-
forms into discrete representations. A speech en-
coder is used to extract continuous representa-
tions that are then transformed into discrete se-
quences through vector quantization. Formally,
letX ∈ Rdenote the domain of audio sam-
ples, a waveform is therefore a sequence of sam-
plesx= (x1, . . . , x T), where xt∈ X for all
1≤t≤T. An encoder F:Xm→Rdtrans-
forms windows of samples of width mintoddi-
mensional continuous frame representations. Ap-
plying Ftoxyields a sequence of frame represen-
tations z= (z1, . . . , z T′), where usually T′< T.
Subsequently, a k-means algorithm is applied to
the encoder output to generate a sequence of dis-
crete speech tokens u= (u1, . . . , u T′), where
ui∈ {1, . . . , K }for1≤i≤T′, and Kis the
size of the vocabulary.
Language models aim to learn the joint proba-
bility of token sequences P(w1, . . . , w n). By the
chain rule of probability, the probability of a se-
quence can be computed as a product of its condi-
tional probabilities:
P(w1, . . . , w n) =nY
i=1P(wi|w1, . . . , w i−1)(1)
Neural LMs, parameterized by θ, are neural
networks that model the conditional probabilities
Pθ(wi|M(w1, . . . , w i−1)), where Mis a represen-tation of the previous tokens. The network is opti-
mized to minimize the negative log-likelihood of
observed ground truth sequences:
L=−nX
i=1Pθ(wi|M(w1, . . . , w i−1)) (2)
Nowadays, the network is typically a transformer
(Vaswani et al., 2017). LLMs are large transformer
LMs trained on large text corpora (billions of pa-
rameters and tokens). SLMs are neural LMs ap-
plied to speech tokens u.
2.2 Scaling laws for neural language models
The performance of deep learning models often
behaves predictably as a function of model size,
dataset size, and compute (Hestness et al., 2017).
Kaplan et al. (2020) showed that the loss L(Equa-
tion 2) of large neural LMs scales with a power law
behavior as a function of these three scale factors:
L(C)∝Cγ, L(N)∝Nα, L(D)∝Dβ(3)
Where Cis the amount of compute (in FLOPS), N
is the number of parameters of the model, and Dis
the number of training tokens.
Building upon their work, Hoffmann et al. (2022)
proposed a parametric function to model the final
loss of neural LMs trained for a single epoch as a
function of NandD:
ˆL(N, D ) =E+A
Nα+B
Dβ(4)
Where the first term is the loss for an ideal LM, and
should correspond to the entropy of the distribution
of token sequences. The second term captures the
approximation error that results from using a neural
network with Nparameters to approximate the
ideal generative process. The final term reflects
that the model is not trained to convergence, as a
finite number of optimization steps are performed
on a sample of size Dfrom the real distribution.
Hoffmann et al. (2022) aimed to solve the prob-
lem of optimal allocation of resources given a fixed
compute budget Cavail. They proposed to approx-
imate the compute needed to train a transformer
LM with Nparameters on Dtokens as C≈6ND.
Then, the problem of optimal allocation of compute
for model size and training data is:
min
N,DˆL(N, D ),s.t. 6ND=Cavail (5)For which the solution is:
Nopt(C) =GC
6a
Dopt(C) =1
GC
6b(6)
With:
G=αA
βB 1
α+β
, a=β
α+β,andb=α
α+β
Muennighoff et al. (2023) generalized Equation
4 to the case of multi-epoch training by replacing
DandNwith terms corresponding to the effective
dataD′and effective model parameters N′:
ˆL(N′, D′) =E+A
N′α+B
D′β(7)
Where D′≤Dis the number of effective training
tokens, assuming that the value of repeated tokens
decays exponentially. Similarly, they note that over-
sized models offer diminishing returns per param-
eter, as excess parameters learn the same features
and do not add value (in the extreme). They pro-
pose an exponential decay model for them, yielding
a number of effective parameters N′≤N. They
derived the expressions for D′andN′as:
D′=UD+UDR∗
D(1−e−RD
R∗
D)
N′=UN+UNR∗
N(1−e−RN
R∗
N)(8)
Where UDis the number of unique tokens used,
RD=D
UD−1is the number of repetitions (0 for
a single epoch), UNis the number of parameters
needed to optimally fit UDaccording to Equation 6,
RN=N
UN−1is the number of excess parameters,
andR∗
DandR∗
Nare constants.
The constants E,A,B,α,β,R∗
DandR∗
Ncan
be estimated empirically by fitting Equation 4 or
7 to a set of tuples (N, D, R N, RD, L)obtained
from training experiments with different budgets.
3 Experimental setup
3.1 Models and training
We adhere to the framework described in Section
2.1. For the speech tokenizer, we use a pre-trained
HuBERT model (Hsu et al., 2021) with frame-rate
of 25 Hz as the speech encoder F, and a vocabulary
size of K= 500 . This setup reports the best per-
formance among publicly available models (HassidSIZE LAYERS MODEL DIM . H EADS
20M 6 512 8
85M 12 768 12
155M 12 1024 16
309M 24 1024 16
823M 16 2048 32
Table 1: Models description.
et al., 2023). For the SLMs we use the Llama archi-
tecture (Touvron et al., 2023) with context window
of 2050 tokens. Table 1 describes the model sizes
used in our experiments. For the LLMs, we use the
Pythia suite of pre-trained LLMs (Biderman et al.,
2023), ranging in size from 14M to 6.9B param-
eters (we do not use the largest 12B model), and
trained with ∼300B tokens.
All SLMs are optimized using AdamW
(Loshchilov and Hutter, 2019) with weight decay
of 0.1, maximum learning rate of 5e-4, half-cycle
cosine decay learning rate schedule to 5e-5, and
a warm-up initial stage of max(100 ,0.01niters)
steps, where niters is the number of training steps,
which varies for each experiment according to the
data budget. We use batch sizes of 64, 128, 256
and 512 for the models with 20M, 85M, 155M and
309M, and 828M parameters, respectively.
To fit the constants in Equations 4 and 7, we
adopt the approaches of Hoffmann et al. (2022)
and Muennighoff et al. (2023), utilizing the Huber
loss with δ= 0.03as the error function and L-
BFGS as optimizer. Following Muennighoff et al.
(2023), we first fit the parameters E,A,B,α, and
βusing the single-epoch runs, and afterwards fit
R∗
DandR∗
Nusing the multi-epoch runs.
3.2 Evaluation
We use the SBLIMP task (Nguyen et al., 2020)
to measure syntactic performance. In SBLIMP ,
the model is presented with a matched pair of
speech segments, grammatical and ungrammati-
cal sentences. The objective is to assign higher
probability to the grammatical sentence.
To evaluate semantic understanding we use the
spoken STORY CLOZE benchmark from Hassid
et al. (2023), a spoken version of the StoryCloze
textual benchmark (Mostafazadeh et al., 2016),
which consists of 4k five-sentence commonsense
stories. In StoryCloze, the model receives as in-
put the first four sentences of a story, and has to
assign higher probability to the correct final sen-
tence than to an adversarial negative sample. TheDATASET HOURSHUBERT
TOKENSUNIGRAM
LIBRISPEECH 960 67M 38M
LIBRILIGHT 53K 3.74B 2.11B
SWC 1 K 32M 19M
TEDLIUM 1.6K 0.11B 67M
PEOPLE 7K 0.48B 0.29B
VOXPOPULI 24K 1.64B 1.08B
STINYSTORIES 72K 4.82B 2.71B
TOTAL 160 K 10.89B 6.31B
Table 2: Datasets statistics. The UNIGRAM column cor-
responds to the dataset of HuBERT tokens compressed
through unigram tokenization.
spoken benchmark comes in two versions: Story
Cloze and Topic Cloze. The difference between
them lies in how the negative sample is generated.
Spoken Story Cloze uses the same samples as the
textual benchmark, which require commonsense
reasoning to distinguish from the real ending. In
Topic Cloze, the negatives are randomly sampled
from the whole dataset, and therefore measures the
ability of the model to stay on topic.
Regarding upstream performance, in all cases we
report and use for the parametric fits the average
loss (Equation 2) on the test set.
3.3 Data
3.3.1 Datasets
We use a collection of publicly available English
speech datasets for training: LibriSpeech (Panay-
otov et al., 2015), LibriLight (Kahn et al., 2020),
SWC (Baumann et al., 2019), Tedlium (Hernandez
et al., 2018), People’s Speech (Galvez et al., 2021),
and V ox Populi (Wang et al., 2021b); and a novel
dataset: STINYSTORIES , a spoken version of the
Tiny Stories dataset (Eldan and Li, 2023) that we
synthesized using the single-speaker TTS system
provided by Wang et al. (2021a). Tiny Stories is
a synthetic text corpus of short stories designed
to boost commonsense reasoning in neural LMs.
We propose STINYSTORIES because we hypoth-
esize that the semantic understanding that tasks
such as Story Cloze measure is hard to acquire
from commonly used speech datasets. Consider
for instance the audiobooks in LibriLight. The
data has long-range dependencies spanning multi-
ple pages, whereas our SLMs can ingest roughly a
dozen sentences of spoken text in their context win-
dow. Other datasets, which were mainly designed
to serve as training data for automatic speech recog-nition systems, consist of too small fragments of au-
dio that lack meaningful causal structure. STINYS-
TORIES consists of full stories with causal structure
that fit within the context window of our SLMs.
We do not include samples from STINYSTORIES
in our test set, as we intend to use our test loss as
measure of the quality with which SLMs model nat-
ural language, not synthetic one. For other datasets
we use the defined held-out sets for testing. In cases
where a held-out set is not defined, we randomly
sampled 1% of the data to serve as test set. See
Table 2 for dataset sizes.
3.3.2 Data budgets
In order to have a representative set of sam-
ples to fit Equations 4 and 7, for each model
size, we performed training runs with a ratio of
training tokens Dto parameters N:D/N ∈
{2,4,8,16,32,64,128}. This setup yields single-
epoch and multi-epoch runs for the larger models
(e.g. for the model with 155M parameters the maxi-
mum number of training tokens corresponds to 1.82
epochs), but not for the smaller models (e.g. for the
model with 85M parameters the maximum number
of training tokens corresponds to 0.99 epochs). To
have better samples to fit Equation 7, we did addi-
tional experiments so that for each model size there
were runs with training epochs in {2,4,8,10}.
4 Results
4.1 Gains from sTinyStories
In order to determine if STINYSTORIES meaning-
fully contributes to the semantic understanding
of SLMs, we compare the performance on Topic
Cloze and Story Cloze of models trained on one
epoch of the union of LibriSpeech and LibriLight,
against models trained on an equivalent amount
ofSTINYSTORIES tokens. Figure 3 shows the ob-
tained results. Models trained on STINYSTORIES
consistently outperform those trained on audio-
books across all model scales. A factor that could
contribute to the observed performance gain is the
match between training and evaluation speakers, as
both STINYSTORIES and Story Cloze were synthe-
sized using the single-sepaker TTS from Wang et al.
(2021a). However, we believe this to be unlikely
as the speech tokenizer we use likely captures little
speaker-specific information (Nguyen et al., 2023).
To isolate the potential impact of speaker mismatch
between training and evaluation data, we created
a multi-speaker version of the Story Cloze bench-
Figure 3: Gains from synthetic data on downstream
semantic performance of SLMs. Pre-training on sTinyS-
tories yields consistent improvements on semantic un-
derstanding relative to pre-training on audiobooks (Lib-
riSpeech plus LibriLight). Performance gains hold for
mismatched train and test speakers.
mark using Bark TTS1, and repeat the evaluations.
The results, also shown in Figure 3, indicate that
even with mismatched train and test speakers train-
ing on STINYSTORIES yields performance gains.
4.2 Scaling laws
We trained multiple SLMs for each model size with
different data budgets as described in Section 3.3.2.
The resulting learning curves for single-epoch runs
are presented in Figure 1 as a function of compute,
and show that the envelope of minimal loss per
FLOP follows a power law.
4.2.1 Downstream scaling with compute
We analyze the relationship between the upstream
and linguistic downstream performance in SLMs
and LLMs. Figure 4 shows the obtained results.
Syntactic and semantic downstream metrics before
saturation are strongly correlated with the upstream
test loss in both LLMs and SLMs. Therefore, the
envelope of maximum downstream performance
per FLOP also follows a power law, i.e. for a down-
stream performance function Q,Q∝Cγq. The
power laws for the different performance metrics
are depicted in Figure 2 and the exponents pre-
sented in Table 3.
1https://github.com/suno-ai/barkFigure 4: Correlation between downstream linguistic performance and test loss for LLMs and SLMs. Syntactic
(BLIMP) and semantic (Topic Cloze and Story Cloze) metrics are strongly linearly correlated with the upstream test
loss before saturation.
MODALITYγq
BLIMP TCLOZE S CLOZE
TEXT 0.066 0.039 0.046
SPEECH 0.021 0.025 0.017
Table 3: γqpower law coefficients of downstream per-
formance with compute as depicted in Figure 2.
These results allow us to compare the efficiency
with scale of LLMs and SLMs. For each metric,
we can interpret the ratio between the γqexponents
of the power laws of LLMs and SLMs as the rel-
ative efficiency with scale. For BLIMP, the ratio
is0.066
0.021= 3.14, indicating that for an increase in
compute ∆Cyielding a ∆Qin LLM’s syntactic
performance, SLMs require 103.14∆Cto get the
same ∆Q. Similarly, for Topic Cloze and Story
Cloze the ratios are 1.56and2.7, respectively.
4.2.2 Scaling with parameters and tokens
We fitted the functions from Equations 4 and 7 to
our data using the procedure described in Section
3.1. We present the empirically fitted scaling law
parameters and compare them to the ones obtained
for text by Muennighoff et al. (2023) in Table 4.
From Equation 6, Nopt∝CaandDopt∝Cb. For
both modalities a≈b≈0.5, suggesting that as
compute increases, model size and data should be
scaled equally for optimal performance. Contrary
to text, R∗
N> R∗
D, indicating that repeated tokens
decay faster than excess parameters (albeit both
slower than in text). Therefore, in SLMs, compute
allocated to parameters should scale faster than
compute allocated for epochs.E A B α β R∗
NR∗
D
TEXT
MUENNIGHOFF ET AL .1.87 521 1488 0.35 0.35 5.31 15.4
SPEECH 1.73 13.9 39.8 0.25 0.24 31.0 25.0
SPEECH
(UNIGRAM )1.42 3.85 8.90 0.15 0.16 - -
Table 4: Scaling law parameters fit to Equations 4 and 7
for different language tokenizations.
4.3 Unigram tokenization
As mentioned in Section 3.3, we believe that the
limited context window of SLMs could cripple their
ability to model the long-range dependencies in
language required for causal reasoning. Seeking
to mitigate this limitation, we apply unigram to-
kenization to shorten the length of speech token
sequences. We use the SentencePiece tokenizer
(Kudo and Richardson, 2018) with a vocabulary
size of 5000. We choose the vocabulary size on
the scale of previous works that have used simi-
lar tokenization strategies for speech applications
(Chang et al., 2023). The resulting dataset sizes
after compression are presented in Table 2.
We train a set of Speech LMs on the compressed
datasets, with model sizes up to 309M parame-
ters and data budgets ranging from 740M to 6.31B
tokens. We analyze the scaling behavior of the
upstream and downstream metrics and compare
it with SLMs trained on raw HuBERT speech to-
kens in Figure 5. SLMs trained on unigram com-
pressed speech tokens show similar upstream scal-
ing with compute, but worse downstream scaling.
Notably, the performance on the StoryCloze bench-
mark does not seem to scale with compute.
We fitted the function from Equation 4 to the
results obtained on the compressed dataset. Table 4Figure 5: Comparison of the scaling behavior of SLMs trained on raw speech tokens and unigram compressed
tokens. Axes are in logarithmic scale. The upstream loss of SLMs trained on unigram tokens scales better with
compute, but downstream performance scales worse. Notably, the Story Cloze metric for SLMs trained on unigram
tokens does not seem to improve with increased compute.
presents the resulting scaling law parameters. Sim-
ilar to the previous findings, for a given compute
budget, scaling model size and training data equally
is optimal for performance. Due to the poor down-
stream results obtained with unigram tokenization
and the lack of sufficient compute resources, we
did not perform multi-epoch training experiments.
5 Related work
Previous works have studied the scaling behavior
of neural networks on speech applications. Droppo
and Elibol (2021) showed that acoustic models
trained with an auto-predictive coding loss follow
similar power laws to those observed in neural LMs.
Aghajanyan et al. (2023) used the scaling laws from
Hoffmann et al. (2022) to model the scaling behav-
ior of the upstream loss of neural LMs on multiple
modalities, including speech. They used a speech
tokenizer with higher framerate (50 Hz) and vo-
cabulary size ( K= 2000 ) than the one we used
(Section 3.1). Such fine-grained tokenizers capture
a lot of the paralinguistic information in speech
(Nguyen et al., 2023). Therefore, their speech to-
kens can be considered almost a different modality
due to the acoustic variance. Furthermore, they do
not study the behavior with scale of downstream
performance. In this work, we focus on the linguis-
tic content of the signal. As reported by Hassid
et al. (2023), our speech tokenizer performs best
on downstream linguistic applications, and is there-
fore a more suitable choice to study the scaling
behavior of the linguistic performance of SLMs.
This paper is perhaps most closely related to the
work of Hassid et al. (2023). We largely follow
their setup in terms of model architecture and eval-
uation metrics. Similar to our work, they reported
improved linguistic downstream performance with
scale in SLMs. However, they did not characterize
their scaling behavior. Our models of the scalingbehavior of SLMs allow practitioners to determine
the compute needed to attain a specific loss, syntac-
tic and/or semantic performance; and the optimal
allocation of that compute with respect to param-
eters and tokens. To the best of our knowledge,
we are the first to model the scaling properties of
downstream linguistic performance in SLMs, and
to study the scaling of the considered downstream
metrics on text-based LLMs. This enables a com-
parison between the two modalities in terms of
scaling efficiency.
6 Discussion
6.1 On our results
Our work showed that the upstream and down-
stream linguistic performance of our current meth-
ods for GSLM scales predictably with compute.
This suggests that with sufficient computational
resources, the goal of the textless NLP project
of achieving neural LMs trained exclusively on
speech, and matching the linguistic proficiency of
their text-based counterparts, is achievable. The
cost of such models could be prohibitive though, as
we estimate that they will require up to three orders
of magnitude more compute than a text-based LLM
to achieve equivalent performance. In this regard,
recent methods that leverage transfer learning from
text-based LLMs (Hassid et al., 2023; Zhang et al.,
2023; Nguyen et al., 2024) are likely to be a bet-
ter choice to achieve highly performant generative
speech models. These hybrid text-speech genera-
tive models often enable cross-modal applications
such as ASR or TTS. However, it remains to be
seen how knowledge transfer from LLMs performs
when the speech data is in a different language than
the one the LLM was trained on. If there is no sig-
nificant cross-lingual knowledge transfer between
text and speech modalities, SLMs could still be anattractive choice for low-resource languages.
We explored the use of synthetic data and coarser
tokenization to increase the semantic abilities of
SLMs. Our synthetic dataset improved seman-
tic performance, but using a coarser tokenization
led to overall degradation of downstream perfor-
mance. We do not have yet an hypothesis for why
coarser tokens degrade performance, as this seems
counter-intuitive, and contradicts the findings on
other speech applications (Chang et al., 2023). We
leave this as an interesting issue to address in fu-
ture work. Moreover, we believe that working on
methods that allow to increase the information den-
sity per context-window of SLMs holds promise to
improve their scaling behavior.
6.2 On limitations
Any extrapolation from our models of the scaling
behavior of SLMs should be considered optimistic
for the following reasons:
1.The LLMs from the Pythia suite that we used
in this study are likely overtrained (all mod-
els were trained with ∼300B tokens). Opti-
mally trained LLMs (according to Equation
6) should show better performance with scale,
and therefore widen the gap with the scaling
efficiency of SLMs.
2.The envelope of minimal loss per FLOP (Fig-
ure 1) might show a slight negative curvature
at larger scale (Hoffmann et al., 2022), reduc-
ing the scaling efficiency.
3.Our models for downstream performance ig-
nore the fact that the metrics saturate. As ob-
served in text LLMs, the improvements with
scale slow down as performance approaches
the saturation value. It is likely that, due to
saturation, the compute required to yield a
particular performance will be larger than pre-
dicted. Moreover, due to the lower density of
linguistic information per context window in
SLMs relative to LLMs, the saturation values
of the metrics may be lower for SLMs.
7 Conclusions
We have trained a large set of SLMs of different
sizes and on different data budgets. Using the col-
lected data from those experiments, we studied
the scaling properties of their upstream and down-
stream performance using recently proposed mod-
els of scaling laws for neural LMs. The obtainedmodels of the scaling behavior allow practioners to
optimally allocate compute to attain a specific loss,
syntactic and/or semantic performance. We showed
that the pre-training loss and downstream linguistic
performance of SLMs and LLMs is highly cor-
related, and that they both scale predictably ac-
cording to power laws. This predictable behavior
allowed us to compare the scaling properties of
SLMs and LLMs, from which we established that
the linguistic abilities of SLMs scale up to three
orders of magnitude more slowly than those of
LLMs. Additionally, we proposed a new speech
dataset, STINYSTORIES , and showed that its use
during pre-training improves downstream semantic
performance in SLMs. Finally, we explored the
use of coarser speech tokenizations as a method to
increase the ability of SLMs to model long-range
dependencies. However, our results suggest that
this is detrimental to downstream performance.
Acknowledgements
We are grateful to the French National Research
Agency for their support through the ANR-20-
CE23-0012-01 (MIM) grant, and the Institute of
Convergence ILCB, supported by grants from
France 2030 (ANR-16-CONV-0002) and the Ex-
cellence Initiative of Aix-Marseille University
(A*MIDEX). This work was granted access to the
HPC resources of GENCI-IDRIS under the alloca-
tion AD011014044.
References
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning
Hsu, Karen Hambardzumyan, Susan Zhang, Stephen
Roller, Naman Goyal, Omer Levy, and Luke Zettle-
moyer. 2023. Scaling laws for generative mixed-
modal language models. In Proceedings of the
40th International Conference on Machine Learn-
ing, ICML’23. JMLR.org.
Timo Baumann, Arne Köhn, and Felix Hennig. 2019.
The spoken wikipedia corpus collection: Harvesting,
alignment and an application to hyperlistening. Lang.
Resour. Eval. , 53(2):303–329.
Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-
hammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, Aviya Skowron, Lintang
Sutawika, and Oskar Van Der Wal. 2023. Pythia:
a suite for analyzing large language models across
training and scaling. In Proceedings of the 40th Inter-
national Conference on Machine Learning , ICML’23.
JMLR.org.Zalán Borsos, Raphaël Marinier, Damien Vincent, Eu-
gene Kharitonov, Olivier Pietquin, Matt Sharifi,
Dominik Roblek, Olivier Teboul, David Grangier,
Marco Tagliasacchi, and Neil Zeghidour. 2023. Au-
diolm: A language modeling approach to audio gen-
eration. IEEE/ACM Transactions on Audio, Speech,
and Language Processing , 31:2523–2533.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901.
Xuankai Chang, Brian Yan, Kwanghee Choi, Jeeweon
Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Ji-
atong Shi, Jinchuan Tian, Shinji Watanabe, Yuya
Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei
Cheng, Pavel Denisov, Kohei Saijo, and Hsiu-Hsuan
Wang. 2023. Exploring speech recognition, transla-
tion, and understanding with discrete speech units: A
comparative study.
Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng
Chiu, James Qin, Ruoming Pang, and Yonghui Wu.
2021. w2v-bert: Combining contrastive learning
and masked language modeling for self-supervised
speech pre-training. In 2021 IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU) ,
pages 244–250.
J. Droppo and O. Elibol. 2021. Scaling laws for acoustic
models. In Interspeech 2021 .
Ronen Eldan and Yuanzhi Li. 2023. Tinystories: How
small can language models be and still speak coherent
english?
Daniel Galvez, Greg Diamos, Juan Manuel Ciro Tor-
res, Juan Felipe Cerón, Keith Achorn, Anjali Gopi,
David Kanter, Max Lam, Mark Mazumder, and Vi-
jay Janapa Reddi. 2021. The people’s speech: A
large-scale diverse english speech recognition dataset
for commercial usage. In Thirty-fifth Conference on
Neural Information Processing Systems Datasets and
Benchmarks Track (Round 1) .
Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat,
Alexis Conneau, Felix Kreuk, Jade Copet, Alexan-
dre Défossez, Gabriel Synnaeve, Emmanuel Dupoux,
Roy Schwartz, and Yossi Adi. 2023. Textually pre-
trained speech language models. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
François Hernandez, Vincent Nguyen, Sahar Ghannay,
Natalia Tomashenko, and Yannick Estève. 2018. Ted-
lium 3: Twice as much data and corpus repartition forexperiments on speaker adaptation. In Speech and
Computer , pages 198–208, Cham. Springer Interna-
tional Publishing.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gre-
gory F. Diamos, Heewoo Jun, Hassan Kianinejad,
Md. Mostofa Ali Patwary, Yang Yang, and Yanqi
Zhou. 2017. Deep learning scaling is predictable,
empirically. CoRR , abs/1712.00409.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Si-
monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
and Laurent Sifre. 2022. Training compute-optimal
large language models.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-
rahman Mohamed. 2021. HuBERT: Self-supervised
speech representation learning by masked prediction
of hidden units. IEEE/ACM Trans. Audio Speech
Lang. , 29:3451–3460.
J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu,
P.E. Mazaré, J. Karadayi, V . Liptchinsky, R. Col-
lobert, C. Fuegen, T. Likhomanenko, G. Synnaeve,
A. Joulin, A. Mohamed, and E. Dupoux. 2020. Libri-
light: A benchmark for asr with limited or no super-
vision. In ICASSP 2020 - 2020 IEEE International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP) , pages 7669–7673.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. CoRR ,
abs/2001.08361.
Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi
Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen,
Morgane Riviere, Abdelrahman Mohamed, Em-
manuel Dupoux, and Wei-Ning Hsu. 2022. Text-free
prosody-aware generative spoken language modeling.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 8666–8681, Dublin, Ireland.
Association for Computational Linguistics.
Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.
Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,
Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh
Nguyen, Jade Copet, Alexei Baevski, Abdelrahman
Mohamed, and Emmanuel Dupoux. 2021. On gen-
erative spoken language modeling from raw audio.Transactions of the Association for Computational
Linguistics , 9:1336–1354.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In International Confer-
ence on Learning Representations .
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A corpus
and cloze evaluation for deeper understanding of
commonsense stories. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 839–849, San Diego,
California. Association for Computational Linguis-
tics.
Niklas Muennighoff, Alexander M Rush, Boaz Barak,
Teven Le Scao, Nouamane Tazi, Aleksandra Piktus,
Sampo Pyysalo, Thomas Wolf, and Colin Raffel.
2023. Scaling data-constrained language models.
InThirty-seventh Conference on Neural Information
Processing Systems .
Tu Anh Nguyen, Maureen de Seyssel, Patricia
Rozé, Morgane Rivière, Evgeny Kharitonov, Alexei
Baevski, Ewan Dunbar, and Emmanuel Dupoux.
2020. The zero resource speech benchmark 2021:
Metrics and baselines for unsupervised spoken lan-
guage modeling. CoRR , abs/2011.11588.
Tu Anh Nguyen, Wei-Ning Hsu, Antony D’Avirro,
Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Re-
mez, Jade Copet, Gabriel Synnaeve, Michael Has-
sid, Felix Kreuk, Yossi Adi, and Emmanuel Dupoux.
2023. Expresso: A Benchmark and Analysis of Dis-
crete Expressive Speech Resynthesis. In Proc. IN-
TERSPEECH 2023 , pages 4823–4827.
Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R.
Costa-jussa, Maha Elbayad, Sravya Popuri, Paul-
Ambroise Duquenne, Robin Algayres, Ruslan Mav-
lyutov, Itai Gat, Gabriel Synnaeve, Juan Pino, Benoit
Sagot, and Emmanuel Dupoux. 2024. SpiRit-LM:
Interleaved Spoken and Written Language Model.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-
jeev Khudanpur. 2015. Librispeech: An asr corpus
based on public domain audio books. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 5206–5210.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Pro-
cessing Systems , volume 30.Changhan Wang, Wei-Ning Hsu, Yossi Adi, Adam
Polyak, Ann Lee, Peng-Jen Chen, Jiatao Gu, and
Juan Pino. 2021a. fairseq sˆ2: A scalable and inte-
grable speech synthesis toolkit. In Proceedings of
the 2021 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations ,
pages 143–152, Online and Punta Cana, Dominican
Republic. Association for Computational Linguistics.
Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu,
Chaitanya Talnikar, Daniel Haziza, Mary Williamson,
Juan Pino, and Emmanuel Dupoux. 2021b. V oxPop-
uli: A large-scale multilingual speech corpus for rep-
resentation learning, semi-supervised learning and
interpretation. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 993–1003, Online. Association for
Computational Linguistics.
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan,
Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023.
SpeechGPT: Empowering large language models
with intrinsic cross-modal conversational abilities.
InFindings of the Association for Computational
Linguistics: EMNLP 2023 , pages 15757–15773, Sin-
gapore. Association for Computational Linguistics.